{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks with Sequential Data \n",
    "consider a task to fit the data $t = 1, 2, 3, \\dots,$ and $x_t = x(t) = \\sin(0.1 t)$ (you do not know it in advance), given samples of $t$ from $1$ to $n$, the task is to predict the value of $x(t)$, when $t = n+1$.\n",
    "\n",
    "- Method 1: find the function $x(t)$ directly\n",
    "    - asumme $x(t)$ is a polynomial and fit it by Least Square Method.\n",
    "    - since Neural Network can approximate any functions in theory, fit the data by a fully connected neural network / feed forward network.\n",
    "- Method 2: find the recurrence relations of $x_t$\n",
    "    - Actually $x_t \\approx 1.99 x_{t-1} - x_{t-2}$ \n",
    "    - The model only need 2 parameters, however, yields very good results! \n",
    "\n",
    "- Feed forward networks are not so efficent to deal with tasks with sequential data. \n",
    "    - do not make use of the sequential relationship explicitly.\n",
    "    - large number of parameters.\n",
    "    - fixed size vector input and output, but the tasks may require flexible size.\n",
    "\n",
    "\n",
    "![task.png](img/task.png)\n",
    "\n",
    "\n",
    "- tasks\n",
    "    - one to one example \n",
    "        - e.g slot filling: classify the words, 0: irrelevant, 1: destination, 2:departure, 3 time.\n",
    "            - I will go to SZ this Sat. ->[ 0, 0, 0, 0, 1, 0, 3] \n",
    "            - I will leave HK this Sat. ->[ 0, 0, 0, 0, 2, 0, 3] \n",
    "        - predict the type of word in a sentence: -> verb, noun\n",
    "    - one to many\n",
    "        - image caption \n",
    "    - many to one\n",
    "        - Sentiment Analysis \n",
    "            - \"The paper, Learning to l Gradient by Gradient Decent, is very very hard to understand.\"-> negative \n",
    "            - \"LSTM is not something difficult to learn.\"-> positive\n",
    "\n",
    "    - many to many\n",
    "        - translation\n",
    "        \n",
    "We will focus on the basic one to one cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional method\n",
    "\n",
    "- Time series prediction task\n",
    "\n",
    "![timeS.png](img\\timeS.png)\n",
    "- Traditional method e.g  The Autoregressive–moving-average model\n",
    "\n",
    "$$X_t = c + \\varepsilon_t + \\sum_i^p \\phi_i X_{t-i} + \\sum_i^q \\theta_i \\varepsilon_{t-i} $$\n",
    "\n",
    "- Assume that $X_t$ is subject to the model ARMA(p, q) first, then evaluate the parameters by fitting the data. \n",
    "\n",
    "    - limited by the linear form of the model if the true recurrence relations are nonlinear\n",
    "        - $X_t = f(X_{t-1}, X_{t-2}, \\dots )$\n",
    "\n",
    "## recurrent networks\n",
    "- DNNs, one layer forward: $x_i \\rightarrow$  after apply affine transfomation: $ W x_i + b \\rightarrow$ then the nonlinearity $h_i = \\sigma( W x_i +b ) \\rightarrow$ the next layer \n",
    "- RNNs, one layer forward: $x_t \\rightarrow$  when applying affine transfomation, also use the information of previous hidden state: $ U x_t + V h_{t-1} + b \\rightarrow$ then the nonlinearity $h_i = \\sigma( U x_t + V h_{t-1} + b) \\rightarrow$ the next layer\n",
    "![RNN.svg](img\\RNN.svg) \n",
    "- Viewpoint: RNN can be viewed as networks with \"short-term memory\".\n",
    "    - DNN, given a $x_i$, get output $f(x_i)$\n",
    "    - RNN, given a $x_t$, get output $f(x_t, h_{t-1})$, then the memory $h_{t-1}$ is combined with some information for $x_t$ and updated to $h_t$.\n",
    "\n",
    "- a simple generated example:\n",
    "    - use one layer RNN with 1-d input, 1-d output and no acitivations, no affine transform for the output, to predict the seqence generated by ARMA(1, 0): $X_t = -0.05 + 0.99 X_{t-1} + \\varepsilon_t$\n",
    "        - Input: $X_t$, initialized by $X_0 = h_0 = 0$\n",
    "        - Output: $h_t = W_x X_t + W_h h_{t-1} +b = f( \\phi;x_t, h_{t-1},)$, where $\\phi =\\{W_x, W_h, b\\}$ including all the parameters of the network for short.\n",
    "        - Loss: use $l_2$ loss, $(h_t-X_{t+1})^2$, the total lost is the sum over all the time step, $t= 1, 2, \\dots,T$.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "def getSample(n, x0 = None, ar = np.array([]), ma = np.array([]), c = 0, sig= 1):\n",
    "    p, q = len(ar), len(ma)\n",
    "    if x0 is None: x0 = np.zeros(p)\n",
    "    e = sig * np.random.randn(n+q)\n",
    "    x = np.empty(n + p)\n",
    "    x[:p] = x0[-p:]\n",
    "    for i in range(n):\n",
    "        x[p+i] = c + e[i+q] + x[i:i+p].dot(ar) + e[i:i+q].dot(ma)\n",
    "    return x[p:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 13.382750305320815\n",
      "loss: 0.0008614868967846247\n",
      "loss: 0.0008351518482966048\n",
      "Wx: [[0.92492892]] Wh: [[0.06602186]] b: [-0.04758399]\n",
      "Wx+Wh= [[0.99095079]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = getSample(500, ar = np.array([0.99]),sig=0.03, c= -0.05)\n",
    "\n",
    "import network as nw\n",
    "from loss import Lossl2\n",
    "from optimizer import Gradientdescent\n",
    "N_train, N_test = 300, 100\n",
    "\n",
    "x_train, x_test = x[:N_train], x[N_train:N_train+N_test]\n",
    "y_train, y_test = x[1:N_train+1], x[1+N_train: 1+N_train+N_test]\n",
    "\n",
    "layers = [nw.RnnLayer(1,1, activation = None)]     \n",
    "\n",
    "net = nw.NeuralNetwork(layers, Lossl2, optimizer = Gradientdescent(alpha = 0.03, decay_rate = 0.99, decay_step = 200))\n",
    "net.train(x_train, y_train, max_iter = 900, print_every = 300 , batch_size = None)\n",
    "\n",
    "layers[0].print()\n",
    "print(\"Wx+Wh=\", layers[0].Wx + layers[0].Wh)\n",
    "\n",
    "x_pre = np.zeros(N_train+N_test)\n",
    "\n",
    "x_pre[:N_train] = net.predict((x[:N_train]).reshape(-1)).reshape(-1)\n",
    "for i in range(N_test):\n",
    "    x_pre[i+N_train] = net.predict(x[i+N_train - 1]).reshape(-1)\n",
    "\n",
    "plt.plot(x_pre, c='red')\n",
    "plt.plot(x[1:1+N_train+N_test], c='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "If you read recent papers, you'll see that many people use a variant on the vanilla RNN called Long-Short Term Memory (LSTM) RNNs. Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradients caused by repeated matrix multiplication. LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism as follows.\n",
    "\n",
    "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
    "\n",
    "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i = \\sigma(a_i) \\hspace{2pc}\n",
    "f = \\sigma(a_f) \\hspace{2pc}\n",
    "o = \\sigma(a_o) \\hspace{2pc}\n",
    "g = \\tanh(a_g)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
    "\n",
    "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
    "\n",
    "$$\n",
    "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
    "h_t = o\\odot\\tanh(c_t)\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the elementwise product of vectors.\n",
    "\n",
    "In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task. \n",
    "\n",
    "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$, and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$ so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$\n",
    "\n",
    "\n",
    "![image.png](img\\LSTM.png)\n",
    "Image by François Deloche, via Wikimedia Commons\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn2learn \n",
    "- Observation \n",
    "    - the updation of the iterative optimization method can be viewed as timeseries\n",
    "    - widely used first order methods, e.g. SGD, NAG, RMSprop, ADAM, act to each entries of parameters elementwise.\n",
    "    - e.g ![RMSprop.png](img/RMSprop.png)\n",
    "- Just fill in the blank \n",
    "    - For every intries $\\theta_j$ in the parameters $\\theta$ of the Optimizee $f(\\theta)$ \n",
    "        - Input: a real number $(\\nabla_t)_j = (\\nabla_{\\theta} \\, f(\\theta_t))_j$ \n",
    "        - Output: a real number $g_j$, the returned value of the network $\\mathrm{LSTM\\_Network}(\\phi; (\\nabla_t)_j, h_j)$\n",
    "    - Loss:\n",
    "        - first update the $\\theta_t$ to $\\theta_{t+1}$ by, for every $j$, do $\\theta_j \\leftarrow \\theta_j + g_j$ \n",
    "        - the loss after one time step is just the value of the Optimizee $f(\\theta_{t+1})$ \n",
    "        - the total loss is the sum of loss over all the $T$ time step\n",
    "\n",
    "http://nbviewer.jupyter.org/github/AdrienLE/learning_by_grad_by_grad_repro/blob/master/Grad%5E2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
